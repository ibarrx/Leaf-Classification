{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c35944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120_20240303\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# import librarires\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pickle\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "from custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1497681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(path):\n",
    "    # Get list of all files and directories in the given path\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    # Filter out only directories\n",
    "    directories = [content for content in contents if os.path.isdir(os.path.join(path, content))]\n",
    "    \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840227f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = get_directories('./train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17b1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85280554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./train_data.pickle', 'rb'))\n",
    "val_data = pickle.load(open('./val_data.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14cf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = CustomDataset(train_data)\n",
    "new_val_data = CustomDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6500d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25078b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb47529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a52ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120_20240303\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120_20240303\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "cnn = models.resnet18(pretrained = True)\n",
    "# weight\n",
    "num_ftrs = cnn.fc.in_features # 512\n",
    "print(num_ftrs)\n",
    "\n",
    "cnn.fc= nn.Linear(num_ftrs,len(CATEGORIES)) ## ??\n",
    "cnn.to(device)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d194b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define your data loaders\n",
    "train_loader = torch.utils.data.DataLoader(new_train_data, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(new_val_data, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e52bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 1 - Average Loss: 2.613, Accuracy: 0.00%\n",
      "Epoch 2/3\n",
      "Epoch 2 - Average Loss: 2.439, Accuracy: 0.00%\n",
      "Epoch 3/3\n",
      "Epoch 3 - Average Loss: 2.131, Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "cnn.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Print current epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Iterate over the training dataset\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "        # Move data to the appropriate device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # inputs = inputs.float() / 255.0  # Convert inputs to float format\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print progress every 100 mini-batches\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Mini-batch {i}/{len(train_loader)}, Loss: {loss.item():.3f}, Accuracy: {(100*correct/total):.2f}%\")\n",
    "\n",
    "    # Print average loss and accuracy for the epoch\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {running_loss/len(train_loader):.3f}, Accuracy: {(100*correct/total):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a021275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Validate the model\n",
    "cnn.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # inputs = inputs.float() / 255.0\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = cnn(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        val_total += labels.size(0)\n",
    "        val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {(100*val_correct/val_total):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e4d8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_confusion_matrix(model, loader):\n",
    "#     model.eval()\n",
    "    \n",
    "#     all_predicted = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in loader:\n",
    "#             images = images.float() / 255.0  # Normalize images\n",
    "#             labels = labels.long()  # Convert labels to the appropriate data type\n",
    "            \n",
    "#             outputs, _ = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "#             # Collect predicted labels and true labels for confusion matrix\n",
    "#             all_predicted.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     cm = confusion_matrix(all_labels, all_predicted)\n",
    "    \n",
    "#     return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d83e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a06c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095027c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d6085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707fff19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # training and validating\n",
    "# num_epochs = 3\n",
    "\n",
    "# train_acc_data = []\n",
    "# loss_data = []\n",
    "# val_acc_data = []\n",
    "# val_loss_data = []\n",
    "# epochs_arr = []\n",
    "\n",
    "# train(num_epochs, cnn, loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd5986a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m), gridspec_kw\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwspace\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_acc_data\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(val_acc_data, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain vs Validation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_acc_data' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# performance analysis (loss, accuracy, cm)\n",
    "plt.figure(figsize=(10,5))\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 5), gridspec_kw={'wspace': 0.5})\n",
    "\n",
    "axs[0].plot(train_acc_data, label='Train')\n",
    "axs[0].plot(val_acc_data, label='Val')\n",
    "axs[0].set_title('Train vs Validation Accuracy')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].plot(loss_data, label='Train Loss')\n",
    "axs[1].plot(val_loss_data, label='Val Loss')\n",
    "axs[1].set_title('Train Loss')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58934919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf_cse120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
