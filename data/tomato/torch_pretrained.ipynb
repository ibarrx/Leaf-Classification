{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c35944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# import librarires\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pickle\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "from custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1497681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(path):\n",
    "    # Get list of all files and directories in the given path\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    # Filter out only directories\n",
    "    directories = [content for content in contents if os.path.isdir(os.path.join(path, content))]\n",
    "    \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840227f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = get_directories('./train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17b1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85280554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./train_data.pickle', 'rb'))\n",
    "val_data = pickle.load(open('./val_data.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14cf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = CustomDataset(train_data)\n",
    "new_val_data = CustomDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad6500d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x291c360fbe0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x291c360f970>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data loaders\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(new_train_data, \n",
    "                                          batch_size=10, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(new_val_data, \n",
    "                                          batch_size= 10, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b25078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # design cnn architecture / load pree-trained model\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         '''\n",
    "#         Goes through two layers of 2D convolutions, each followed by a 2x2 max pooling\n",
    "        \n",
    "#         Dimensions:\n",
    "#         1.) Input Shape:          [batch_size, 3, 256, 256]\n",
    "        \n",
    "#         2.) After conv:           [batch_size, 6, 254, 254]\n",
    "#             After max pooling:    [batch_size, 6, 127, 127]\n",
    "        \n",
    "#         3.) After conv:           [batch_size, 12, 125, 125]\n",
    "#             After max pooling:    [batch_size, 12, 62, 62]\n",
    "            \n",
    "#         4.) After flattening:     [batch_size, 12 * 62 * 62]\n",
    "#         '''\n",
    "        \n",
    "#         self.conv1 = nn.Sequential(         \n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=3,              \n",
    "#                 out_channels=6,            \n",
    "#                 kernel_size=3,              \n",
    "#                 stride=1,                   \n",
    "#                 padding=0,                  \n",
    "#             ),                              \n",
    "#             nn.ReLU(),                      \n",
    "#             nn.MaxPool2d(kernel_size=2),    \n",
    "#         )\n",
    "#         self.conv2 = nn.Sequential(         \n",
    "#             nn.Conv2d(6, 12, 3, 1, 0),     \n",
    "#             nn.ReLU(),                      \n",
    "#             nn.MaxPool2d(2),                \n",
    "#         )\n",
    "#         # fully connected layer, output 10 classes\n",
    "#         self.out = nn.Linear(12 * 62 * 62, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         # flatten the output of conv2 to (batch_size, 16 * 8 * 8)\n",
    "#         x = x.view(x.size(0), -1)       \n",
    "#         output = self.out(x)\n",
    "#         return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb47529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn= CNN()\n",
    "# print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a52ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = models.resnet18(pretrained = True)\n",
    "num_ftrs = cnn.fc.in_features\n",
    "\n",
    "cnn.fc= nn.Linear(num_ftrs,len(CATEGORIES))\n",
    "cnn.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a021275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss(); print(loss_func)   \n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr= 1e-4); print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0d83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "def validate(cnn, loader, device, compute_cm=False):\n",
    "    cnn.eval()\n",
    "        \n",
    "    # evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.float().to(device) / 255.0  # Normalize images\n",
    "            labels = labels.long().to(device)  # Convert labels to the appropriate data type\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_loss = total_loss / len(loader)\n",
    "    return val_accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c98a06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(num_epochs, cnn, loaders, device):\n",
    "    cnn.to(device)\n",
    "    cnn.train()\n",
    "    \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    print(\"INITIATING TRAINING...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_array = []\n",
    "        loss_array = []\n",
    "        val_acc_array = []\n",
    "        val_loss_array = []\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            # convert images and labels into float tensors and normalize\n",
    "            images = images.float().to(device) / 255.0\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            print(\"step\", i)\n",
    "            \n",
    "            # measure accuracy and record loss\n",
    "            \n",
    "            train_output = cnn(images) \n",
    "            \n",
    "            pred_y = torch.max(train_output, 1)[1].data.squeeze()\n",
    "            \n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "                    \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()\n",
    "            \n",
    "            # apply gradients             \n",
    "            optimizer.step()  \n",
    "            \n",
    "            # validate model\n",
    "            val_accuracy, val_loss = validate(cnn, loaders['test'], device)\n",
    "            \n",
    "            # storing results\n",
    "            acc_array.append(accuracy)\n",
    "            loss_array.append(loss.item())\n",
    "            val_acc_array.append(val_accuracy)\n",
    "            val_loss_array.append(val_loss)\n",
    "            \n",
    "            # epochs_arr.append((i+1) / total_step)\n",
    "            \n",
    "            if (i+1) % 25 == 0 or (i+1) == total_step:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.4f}, Validation Accuracy: {:.4f}, Validation Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), accuracy, val_accuracy, val_loss))                    \n",
    "            pass\n",
    "        \n",
    "        # Averaging results per epoch\n",
    "        train_acc_data.append(np.mean(acc_array))\n",
    "        loss_data.append(np.mean(loss_array))\n",
    "        val_acc_data.append(np.mean(val_acc_array))\n",
    "        val_loss_data.append(np.mean(val_loss_array))\n",
    "        \n",
    "        # Logging results per epoch\n",
    "        print(\"\\n-----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        print(f\"EPOCH {epoch+1} RESULTS:\")\n",
    "        print(\"AVG TRAIN ACCURACY: {:.4f}, AVG TRAIN LOSS: {:.4f}, AVG VAL ACCURACY: {:.4f}, AVG VAL LOSS: {:.4f}\"\n",
    "              .format(train_acc_data[epoch], loss_data[epoch], val_acc_data[epoch], val_loss_data[epoch]))\n",
    "        print(\"\\n-----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095027c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cm(cnn, loader, device):\n",
    "    cnn.eval()\n",
    "        \n",
    "    # evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.float().to(device) / 255.0  # Normalize images\n",
    "            labels = labels.float().to(device)  # Convert labels to the appropriate data type\n",
    "            \n",
    "            outputs, _ ,_= cnn(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_loss = total_loss / len(loader)\n",
    "    return val_accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c32d6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.figure(figsize=(12,7))\n",
    "#     plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "707fff19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIATING TRAINING...\n",
      "step 0\n",
      "step 1\n",
      "step 2\n",
      "step 3\n",
      "step 4\n",
      "step 5\n",
      "step 6\n",
      "step 7\n",
      "step 8\n",
      "step 9\n",
      "step 10\n",
      "step 11\n",
      "step 12\n",
      "step 13\n",
      "step 14\n",
      "step 15\n",
      "step 16\n",
      "step 17\n",
      "step 18\n",
      "step 19\n",
      "step 20\n",
      "step 21\n",
      "step 22\n",
      "step 23\n",
      "step 24\n",
      "Epoch [1/3], Step [25/1000], Loss: 91.4443, Accuracy: 0.1000, Validation Accuracy: 0.0940, Validation Loss: 2.3351\n",
      "step 25\n",
      "step 26\n",
      "step 27\n",
      "step 28\n",
      "step 29\n",
      "step 30\n",
      "step 31\n",
      "step 32\n",
      "step 33\n",
      "step 34\n",
      "step 35\n",
      "step 36\n",
      "step 37\n",
      "step 38\n",
      "step 39\n",
      "step 40\n",
      "step 41\n",
      "step 42\n",
      "step 43\n",
      "step 44\n",
      "step 45\n",
      "step 46\n",
      "step 47\n",
      "step 48\n",
      "step 49\n",
      "Epoch [1/3], Step [50/1000], Loss: 90.7951, Accuracy: 0.1000, Validation Accuracy: 0.1270, Validation Loss: 2.3130\n",
      "step 50\n",
      "step 51\n",
      "step 52\n",
      "step 53\n",
      "step 54\n",
      "step 55\n",
      "step 56\n",
      "step 57\n",
      "step 58\n",
      "step 59\n",
      "step 60\n",
      "step 61\n",
      "step 62\n",
      "step 63\n",
      "step 64\n",
      "step 65\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\storage.py\", line 899, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\storage.py\", line 257, in _new_shared\n    return cls._new_using_filename_cpu(size)\nRuntimeError: Couldn't open shared file mapping: <0000015A57B2B622>, error code: <1455>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m val_loss_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m epochs_arr \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs, cnn, loaders, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# validate model\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m val_accuracy, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# storing results\u001b[39;00m\n\u001b[0;32m     52\u001b[0m acc_array\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(cnn, loader, device, compute_cm)\u001b[0m\n\u001b[0;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     11\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize images\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Convert labels to the appropriate data type\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\storage.py\", line 899, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n  File \"c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torch\\storage.py\", line 257, in _new_shared\n    return cls._new_using_filename_cpu(size)\nRuntimeError: Couldn't open shared file mapping: <0000015A57B2B622>, error code: <1455>\n"
     ]
    }
   ],
   "source": [
    "# training and validating\n",
    "num_epochs = 3\n",
    "\n",
    "train_acc_data = []\n",
    "loss_data = []\n",
    "val_acc_data = []\n",
    "val_loss_data = []\n",
    "epochs_arr = []\n",
    "\n",
    "train(num_epochs, cnn, loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance analysis (loss, accuracy, cm)\n",
    "plt.figure(figsize=(10,5))\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 5), gridspec_kw={'wspace': 0.5})\n",
    "\n",
    "axs[0].plot(train_acc_data, label='Train')\n",
    "axs[0].plot(val_acc_data, label='Val')\n",
    "axs[0].set_title('Train vs Validation Accuracy')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].plot(loss_data, label='Train Loss')\n",
    "axs[1].plot(val_loss_data, label='Val Loss')\n",
    "axs[1].set_title('Train Loss')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58934919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf_cse120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
