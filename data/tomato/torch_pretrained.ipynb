{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c35944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# import librarires\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pickle\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "from custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1497681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(path):\n",
    "    # Get list of all files and directories in the given path\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    # Filter out only directories\n",
    "    directories = [content for content in contents if os.path.isdir(os.path.join(path, content))]\n",
    "    \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840227f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = get_directories('./train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17b1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85280554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./train_data.pickle', 'rb'))\n",
    "val_data = pickle.load(open('./val_data.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14cf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = CustomDataset(train_data)\n",
    "new_val_data = CustomDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad6500d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x291c360fbe0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x291c360f970>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data loaders\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(new_train_data, \n",
    "                                          batch_size=10, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(new_val_data, \n",
    "                                          batch_size= 10, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b25078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # design cnn architecture / load pree-trained model\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         '''\n",
    "#         Goes through two layers of 2D convolutions, each followed by a 2x2 max pooling\n",
    "        \n",
    "#         Dimensions:\n",
    "#         1.) Input Shape:          [batch_size, 3, 256, 256]\n",
    "        \n",
    "#         2.) After conv:           [batch_size, 6, 254, 254]\n",
    "#             After max pooling:    [batch_size, 6, 127, 127]\n",
    "        \n",
    "#         3.) After conv:           [batch_size, 12, 125, 125]\n",
    "#             After max pooling:    [batch_size, 12, 62, 62]\n",
    "            \n",
    "#         4.) After flattening:     [batch_size, 12 * 62 * 62]\n",
    "#         '''\n",
    "        \n",
    "#         self.conv1 = nn.Sequential(         \n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=3,              \n",
    "#                 out_channels=6,            \n",
    "#                 kernel_size=3,              \n",
    "#                 stride=1,                   \n",
    "#                 padding=0,                  \n",
    "#             ),                              \n",
    "#             nn.ReLU(),                      \n",
    "#             nn.MaxPool2d(kernel_size=2),    \n",
    "#         )\n",
    "#         self.conv2 = nn.Sequential(         \n",
    "#             nn.Conv2d(6, 12, 3, 1, 0),     \n",
    "#             nn.ReLU(),                      \n",
    "#             nn.MaxPool2d(2),                \n",
    "#         )\n",
    "#         # fully connected layer, output 10 classes\n",
    "#         self.out = nn.Linear(12 * 62 * 62, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         # flatten the output of conv2 to (batch_size, 16 * 8 * 8)\n",
    "#         x = x.view(x.size(0), -1)       \n",
    "#         output = self.out(x)\n",
    "#         return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb47529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn= CNN()\n",
    "# print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a52ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\anaconda3\\envs\\leaf_cse120\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = models.resnet18(pretrained = True)\n",
    "num_ftrs = cnn.fc.in_features\n",
    "\n",
    "cnn.fc= nn.Linear(num_ftrs,len(CATEGORIES))\n",
    "cnn.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a021275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss(); print(loss_func)   \n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr= 1e-4); print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0d83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "def validate(cnn, loader, device, compute_cm=False):\n",
    "    cnn.eval()\n",
    "        \n",
    "    # evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.float().to(device) / 255.0  # Normalize images\n",
    "            labels = labels.long().to(device)  # Convert labels to the appropriate data type\n",
    "            \n",
    "            outputs = cnn(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_loss = total_loss / len(loader)\n",
    "    return val_accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c98a06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(num_epochs, cnn, loaders, device):\n",
    "    cnn.to(device)\n",
    "    cnn.train()\n",
    "    \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    print(\"INITIATING TRAINING...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_array = []\n",
    "        loss_array = []\n",
    "        val_acc_array = []\n",
    "        val_loss_array = []\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            # convert images and labels into float tensors and normalize\n",
    "            images = images.float().to(device) / 255.0\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            print(\"step\", i)\n",
    "            \n",
    "            # measure accuracy and record loss\n",
    "            \n",
    "            train_output = cnn(images) \n",
    "            \n",
    "            pred_y = torch.max(train_output, 1)[1].data.squeeze()\n",
    "            \n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "                    \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()\n",
    "            \n",
    "            # apply gradients             \n",
    "            optimizer.step()  \n",
    "            \n",
    "            # validate model\n",
    "            val_accuracy, val_loss = validate(cnn, loaders['test'], device)\n",
    "            \n",
    "            # storing results\n",
    "            acc_array.append(accuracy)\n",
    "            loss_array.append(loss.item())\n",
    "            val_acc_array.append(val_accuracy)\n",
    "            val_loss_array.append(val_loss)\n",
    "            \n",
    "            # epochs_arr.append((i+1) / total_step)\n",
    "            \n",
    "            if (i+1) % 25 == 0 or (i+1) == total_step:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.4f}, Validation Accuracy: {:.4f}, Validation Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), accuracy, val_accuracy, val_loss))                    \n",
    "            pass\n",
    "        \n",
    "        # Averaging results per epoch\n",
    "        train_acc_data.append(np.mean(acc_array))\n",
    "        loss_data.append(np.mean(loss_array))\n",
    "        val_acc_data.append(np.mean(val_acc_array))\n",
    "        val_loss_data.append(np.mean(val_loss_array))\n",
    "        \n",
    "        # Logging results per epoch\n",
    "        print(\"\\n-----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        print(f\"EPOCH {epoch+1} RESULTS:\")\n",
    "        print(\"AVG TRAIN ACCURACY: {:.4f}, AVG TRAIN LOSS: {:.4f}, AVG VAL ACCURACY: {:.4f}, AVG VAL LOSS: {:.4f}\"\n",
    "              .format(train_acc_data[epoch], loss_data[epoch], val_acc_data[epoch], val_loss_data[epoch]))\n",
    "        print(\"\\n-----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095027c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cm(cnn, loader, device):\n",
    "    cnn.eval()\n",
    "        \n",
    "    # evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.float().to(device) / 255.0  # Normalize images\n",
    "            labels = labels.float().to(device)  # Convert labels to the appropriate data type\n",
    "            \n",
    "            outputs, _ ,_= cnn(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_loss = total_loss / len(loader)\n",
    "    return val_accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c32d6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.figure(figsize=(12,7))\n",
    "#     plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "707fff19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIATING TRAINING...\n",
      "step 0\n",
      "step 1\n",
      "step 2\n",
      "step 3\n",
      "step 4\n",
      "step 5\n",
      "step 6\n",
      "step 7\n",
      "step 8\n",
      "step 9\n",
      "step 10\n",
      "step 11\n",
      "step 12\n"
     ]
    }
   ],
   "source": [
    "# training and validating\n",
    "num_epochs = 3\n",
    "\n",
    "train_acc_data = []\n",
    "loss_data = []\n",
    "val_acc_data = []\n",
    "val_loss_data = []\n",
    "epochs_arr = []\n",
    "\n",
    "train(num_epochs, cnn, loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance analysis (loss, accuracy, cm)\n",
    "plt.figure(figsize=(10,5))\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 5), gridspec_kw={'wspace': 0.5})\n",
    "\n",
    "axs[0].plot(train_acc_data, label='Train')\n",
    "axs[0].plot(val_acc_data, label='Val')\n",
    "axs[0].set_title('Train vs Validation Accuracy')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].plot(loss_data, label='Train Loss')\n",
    "axs[1].plot(val_loss_data, label='Val Loss')\n",
    "axs[1].set_title('Train Loss')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58934919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf_cse120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
