{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries needed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os, requests, cv2, random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking directory\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'train' #'data/tomato/train' does not work\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(train_data_dir):\n",
    "    print(f\"The directory {train_data_dir} does not exist.\")\n",
    "else:\n",
    "    print(f\"Found directory {train_data_dir}. Proceeding with loading data...\")\n",
    "    # Proceed with your data loading and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the pictures of tomatoes\n",
    "\n",
    "train_data_dir = './new_dataset/train_output'\n",
    "test_data_dir = './new_dataset/val_output' #this folder will be used for evaluating model's perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this challenge we are using ImageDataGenerator \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.0,\n",
    "                               shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                validation_split=0.3)# specifying the validation split inside the function\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.0,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = train_datagen.flow_from_directory(\n",
    "                                train_data_dir,\n",
    "                                target_size=(224, 224),\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                class_mode='categorical',\n",
    "                                subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = train_datagen.flow_from_directory(\n",
    "                                train_data_dir,\n",
    "                                target_size=(224,224),\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                class_mode='categorical',\n",
    "                                subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle = False) #shuffle will not affect the accuracy of the model, but will affect the computation of some metrics that depend on the order of the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Building a CNN model with a learning rate of 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model layers\n",
    "\n",
    "cnn = models.Sequential()\n",
    "\n",
    "cnn.add(layers.Conv2D (32, kernel_size = (3,3), activation='relu', input_shape = [224, 224,3])),\n",
    "cnn.add(layers.MaxPooling2D(pool_size = (2, 2))),\n",
    "\n",
    "cnn.add(layers.Conv2D(64, (3,3), activation='relu')),\n",
    "cnn.add(layers. MaxPooling2D((2, 2))),\n",
    "\n",
    "cnn.add(layers.Conv2D (64, (3,3), activation='relu')),\n",
    "cnn.add(layers.MaxPooling2D((2, 2))),\n",
    "\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu')), \n",
    "cnn.add(layers.MaxPooling2D((2, 2))),\n",
    "\n",
    "cnn.add(layers. Conv2D (64, (3, 3), activation='relu')), \n",
    "cnn.add(layers.MaxPooling2D((2, 2))),\n",
    "\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu')), \n",
    "cnn.add(layers.MaxPooling2D((2, 2))),\n",
    "\n",
    "cnn.add(layers.Flatten()),\n",
    "\n",
    "cnn.add(layers.Dense(64,activation='relu'))\n",
    "#output layer\n",
    "cnn.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, let's define an Early Stopping to avoid oferfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_accuracy', \n",
    "                   mode = 'max', \n",
    "                   patience = 20, \n",
    "                   verbose = 1, \n",
    "                   restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(x = train_gen,\n",
    "                    callbacks = [es], \n",
    "                    # steps_per_epoch = 7000/32,\n",
    "                    epochs = 100,\n",
    "                    # validation_steps = 3000/32,\n",
    "                    validation_data = val_gen)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cnn.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to save your model for later\n",
    "cnn.save('rembg_training.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions, confusion matrix, metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps_per_epoch = np.math.ceil(test_gen.samples / test_gen.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes\n",
    "predictions = cnn.predict(test_gen, steps=test_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground-truth classes and class-labels\n",
    "true_classes = test_gen.classes\n",
    "class_labels = list(test_gen.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "confusion_matrix = confusion_matrix(test_gen.classes, predicted_classes)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check F1 score, recall etc.\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
