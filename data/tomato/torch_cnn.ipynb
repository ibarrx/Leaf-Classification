{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c35944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librarires\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pickle\n",
    "import os\n",
    "from custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1497681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(path):\n",
    "    # Get list of all files and directories in the given path\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    # Filter out only directories\n",
    "    directories = [content for content in contents if os.path.isdir(os.path.join(path, content))]\n",
    "    \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840227f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = get_directories('./train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85280554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('./train_data.pickle', 'rb'))\n",
    "val_data = pickle.load(open('./val_data.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = CustomDataset(train_data)\n",
    "new_val_data = CustomDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data loaders\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(new_train_data, \n",
    "                                          batch_size=32, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(new_val_data, \n",
    "                                          batch_size=32, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design cnn architecture / load pree-trained model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        '''\n",
    "        Goes through two layers of 2D convolutions, each followed by a 2x2 max pooling\n",
    "        \n",
    "        Dimensions:\n",
    "        1.) Input Shape:          [batch_size, 3, 256, 256]\n",
    "        \n",
    "        2.) After conv:           [batch_size, 6, 254, 254]\n",
    "            After max pooling:    [batch_size, 6, 127, 127]\n",
    "        \n",
    "        3.) After conv:           [batch_size, 12, 125, 125]\n",
    "            After max pooling:    [batch_size, 12, 62, 62]\n",
    "            \n",
    "        4.) After flattening:     [batch_size, 12 * 62 * 62]\n",
    "        '''\n",
    "        \n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=3,              \n",
    "                out_channels=6,            \n",
    "                kernel_size=3,              \n",
    "                stride=1,                   \n",
    "                padding=0,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(6, 12, 3, 1, 0),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(12 * 62 * 62, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 16 * 8 * 8)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb47529",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn= CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(); print(loss_func)   \n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr= 1e-4); print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "def validate(cnn, loader, device, compute_cm=False):\n",
    "    cnn.eval()\n",
    "        \n",
    "    # evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.float().to(device) / 255.0  # Normalize images\n",
    "            labels = labels.long().to(device)  # Convert labels to the appropriate data type\n",
    "            \n",
    "            outputs, _ = cnn(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_loss = total_loss / len(loader)\n",
    "    return val_accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(num_epochs, cnn, loaders, device):\n",
    "    cnn.to(device)\n",
    "    cnn.train()\n",
    "    \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    print(\"INITIATING TRAINING...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_array = []\n",
    "        loss_array = []\n",
    "        val_acc_array = []\n",
    "        val_loss_array = []\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            # convert images and labels into float tensors and normalize\n",
    "            images = images.float().to(device) / 255.0\n",
    "            labels = labels.long().to(device)\n",
    "            \n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            # measure accuracy and record loss\n",
    "            train_output, _ = cnn(images)\n",
    "            pred_y = torch.max(train_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "            \n",
    "\n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()\n",
    "            \n",
    "            # apply gradients             \n",
    "            optimizer.step()  \n",
    "            \n",
    "            # validate model\n",
    "            val_accuracy, val_loss = validate(cnn, loaders['test'], device)\n",
    "            \n",
    "            # storing results\n",
    "            acc_array.append(accuracy)\n",
    "            loss_array.append(loss.item())\n",
    "            val_acc_array.append(val_accuracy)\n",
    "            val_loss_array.append(val_loss)\n",
    "#             epochs_arr.append((i+1) / total_step)\n",
    "            \n",
    "            if (i+1) % 25 == 0 or (i+1) == total_step:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.4f}, Validation Accuracy: {:.4f}, Validation Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), accuracy, val_accuracy, val_loss))                    \n",
    "            pass\n",
    "        \n",
    "        # Averaging results per epoch\n",
    "        train_acc_data.append(np.mean(acc_array))\n",
    "        loss_data.append(np.mean(loss_array))\n",
    "        val_acc_data.append(np.mean(val_acc_array))\n",
    "        val_loss_data.append(np.mean(val_loss_array))\n",
    "        \n",
    "        # Logging results per epoch\n",
    "        print(\"\\n-----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        print(f\"EPOCH {epoch+1} RESULTS:\")\n",
    "        print(\"AVG TRAIN ACCURACY: {:.4f}, AVG TRAIN LOSS: {:.4f}, AVG VAL ACCURACY: {:.4f}, AVG VAL LOSS: {:.4f}\"\n",
    "              .format(train_acc_data[epoch], loss_data[epoch], val_acc_data[epoch], val_loss_data[epoch]))\n",
    "        print(\"\\n-----------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095027c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cm(cnn, loader, device):\n",
    "    cnn.eval()\n",
    "        \n",
    "    # evaluate the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loaders['test']:\n",
    "            images = images.float().to(device) / 255.0  # Normalize images\n",
    "            labels = labels.long().to(device)  # Convert labels to the appropriate data type\n",
    "            \n",
    "            outputs, _ = cnn(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    val_loss = total_loss / len(loader)\n",
    "    return val_accuracy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.figure(figsize=(12,7))\n",
    "#     plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707fff19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [25/313], Loss: 2.2914, Accuracy: 0.0938, Validation Accuracy: 0.2450, Validation Loss: 2.2457\n"
     ]
    }
   ],
   "source": [
    "# training and validating\n",
    "num_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_acc_data = []\n",
    "loss_data = []\n",
    "val_acc_data = []\n",
    "val_loss_data = []\n",
    "epochs_arr = []\n",
    "\n",
    "train(num_epochs, cnn, loaders,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd5986a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# performance analysis (loss, accuracy, cm)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m), gridspec_kw\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwspace\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m})\n\u001b[0;32m      5\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(train_acc_data, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# performance analysis (loss, accuracy, cm)\n",
    "plt.figure(figsize=(10,5))\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 5), gridspec_kw={'wspace': 0.5})\n",
    "\n",
    "axs[0].plot(train_acc_data, label='Train')\n",
    "axs[0].plot(val_acc_data, label='Val')\n",
    "axs[0].set_title('Train vs Validation Accuracy')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].plot(loss_data, label='Train Loss')\n",
    "axs[1].plot(val_loss_data, label='Val Loss')\n",
    "axs[1].set_title('Train Loss')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58934919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf_cse120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
